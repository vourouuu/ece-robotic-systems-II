{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4b152f-28cb-4d3d-8c25-afea2d8bdd28",
   "metadata": {},
   "source": [
    "# Robotic Systems II (ECE-DK904)\n",
    "\n",
    "## Electrical and Computer Engineering Department, University of Patras, Greece\n",
    "\n",
    "**Instructor:** Konstantinos Chatzilygeroudis (costashatz@upatras.gr)\n",
    "\n",
    "## Lab 3\n",
    "\n",
    "### Constrained Optimization with Equality Constraints\n",
    "\n",
    "In constrained optimization with equality constraints we need to solve the following problem:\n",
    "\n",
    "$\\min_{\\boldsymbol{x}} f(\\boldsymbol{x})$\n",
    "\n",
    "$\\quad\\quad\\text{s.t. }h(\\boldsymbol{x}) = 0$\n",
    "\n",
    "where $f(\\boldsymbol{x}) : \\mathbb{R}^N\\to\\mathbb{R}, h(\\boldsymbol{x}): \\mathbb{R}^N\\to\\mathbb{R}^M$.\n",
    "\n",
    "We saw in the class that we can tackle this problem by defining a new function, called the `Lagrangian`:\n",
    "\n",
    "$\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\lambda}) = f(\\boldsymbol{x}) + \\boldsymbol{\\lambda}^Th(\\boldsymbol{x})$\n",
    "\n",
    "where $\\boldsymbol{\\lambda}\\in\\mathbb{R}^M$. Taking the derivative of the Langragian, we get:\n",
    "\n",
    "$\\nabla_{\\boldsymbol{x}}\\mathcal{L} = \\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}) + \\Big(\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big)^T\\boldsymbol{\\lambda} = 0$\n",
    "\n",
    "$\\nabla_{\\boldsymbol{\\lambda}}\\mathcal{L} = h(\\boldsymbol{x}) = 0$\n",
    "\n",
    "which we call the **KKT** conditions. In other words, **these two conditions necessarily have to hold at the optimum!**\n",
    "\n",
    "So how do we proceed in optimizing this? The main idea is to use Newton's method on the Lagrangian! In order to do so, we need to linearize the gradient of the Lagrangian around the current estimate $(\\boldsymbol{x}_k,\\boldsymbol{\\lambda}_k)$:\n",
    "\n",
    "$\\nabla_{\\boldsymbol{x}}\\mathcal{L}(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x}, \\boldsymbol{\\lambda}_k + \\Delta\\boldsymbol{\\lambda}) = \\nabla_{\\boldsymbol{x}}\\mathcal{L}(\\boldsymbol{x}_k, \\boldsymbol{\\lambda}_k) + \\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{x}^2}\\Big|_{\\boldsymbol{x}_k,\\boldsymbol{\\lambda}_k}\\Delta\\boldsymbol{x} + \\underbrace{\\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{x}\\partial\\boldsymbol{\\lambda}}\\Big|_{\\boldsymbol{x}_k,\\boldsymbol{\\lambda}_k}}_{\\Big(\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Big)^T}\\Delta\\boldsymbol{\\lambda} = 0$\n",
    "\n",
    "$\\nabla_{\\boldsymbol{\\lambda}}\\mathcal{L}(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x}, \\boldsymbol{\\lambda}_k + \\Delta\\boldsymbol{\\lambda}) = h(\\boldsymbol{x}_k) + \\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Delta\\boldsymbol{x} = 0$\n",
    "\n",
    "Solving the above we end up with the following equations:\n",
    "\n",
    "$\\begin{bmatrix}\\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{x}^2}\\Big|_{\\boldsymbol{x}_k,\\boldsymbol{\\lambda}_k} & \\Big(\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Big)^T\\\\\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k} & \\boldsymbol{0}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\Delta\\boldsymbol{x}\\\\\\Delta\\boldsymbol{\\lambda}\\end{bmatrix} = \n",
    "\\begin{bmatrix}-\\nabla_{\\boldsymbol{x}}\\mathcal{L}(\\boldsymbol{x}_k, \\boldsymbol{\\lambda}_k)\\\\-h(\\boldsymbol{x}_k)\\end{bmatrix}$\n",
    "\n",
    "We call this the **KKT System**!\n",
    "\n",
    "This is basically one **big linear system**. We can solve with ONE `np.linalg.solve()` call!!\n",
    "\n",
    "**Important detail:**\n",
    "\n",
    "$\\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{x}^2} = \\nabla^2_{\\boldsymbol{x}}f + \\frac{\\partial}{\\partial\\boldsymbol{x}}\\Big[(\\frac{\\partial h}{\\partial\\boldsymbol{x}})^T\\boldsymbol{\\lambda}\\Big]$\n",
    "\n",
    "We usually drop the term $\\frac{\\partial}{\\partial\\boldsymbol{x}}\\Big[(\\frac{\\partial h}{\\partial\\boldsymbol{x}})^T\\boldsymbol{\\lambda}\\Big]$ since in the general case this is a 3D tensor and it's difficult to compute!\n",
    "\n",
    "**ENOUGH TALK!** Let's optimize something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca345c-6974-45f1-86de-b32f3ddded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Linear Algebra\n",
    "import matplotlib.pyplot as plt # Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4c41f-8851-437c-9f1e-e98355f612ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a quadratic objective function and a quadratic constraint\n",
    "Q = np.array(([[0.5, 0.], [0., 1.]]))\n",
    "x_target = np.array([[1.], [0.]])\n",
    "\n",
    "def f(x):\n",
    "    x = np.asarray(x).reshape((2,-1))\n",
    "    return (0.5*(x - x_target).T @ Q @ (x - x_target))[0, 0]\n",
    "\n",
    "def df(x):\n",
    "    x = np.asarray(x).reshape((2,-1))\n",
    "    return (x - x_target).T @ Q\n",
    "\n",
    "def ddf(x):\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63373ed4-fa52-4ccc-8bff-592e042d739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons(x):\n",
    "    x = np.asarray(x).reshape((2,-1))\n",
    "    return x[0, 0]**2 + 2. * x[0, 0] - x[1, 0]\n",
    "\n",
    "def dc(x):\n",
    "    x = np.asarray(x).reshape((2,-1))\n",
    "    return np.asarray([2. * x[0, 0] + 2., -1.]).reshape((1, 2))\n",
    "\n",
    "def ddc(x, l = np.ones((2, 1))):\n",
    "    l = np.asarray(l).reshape((1,-1))\n",
    "    h = np.zeros((2, 2))\n",
    "    h[0, 0] = 2. * l[0,0]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36b031-a404-4c2f-b2e4-ccf5d21dc27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method with Equality Constraints\n",
    "def constrained_newton_step(x0, l0):\n",
    "    N = 2\n",
    "    M = 1\n",
    "    x0 = np.asarray(x0).reshape((N, -1))\n",
    "    l0 = np.asarray(l0).reshape((M, -1))\n",
    "\n",
    "    # Let's compute the KKT system\n",
    "    ### TO-DO: Compute the KKT matrix\n",
    "    ddL_ddx = ddf(x0)\n",
    "    dh_dx = dc(x0)\n",
    "    KKT_system = np.block([[ddL_ddx, dh_dx.T], [dh_dx, 0]])\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Let's compute the target vector\n",
    "    ### TO-DO: Compute the target vector of the KKT system\n",
    "    VxL = df(x0).T + dc(x0).T @ l0\n",
    "    h = cons(x0)\n",
    "    KKT_target_vector = np.block([[-VxL], [-h]])\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Let's solve for Δz\n",
    "    ### TO-DO: Solve for Δz\n",
    "    DeltaZ = np.linalg.solve(KKT_system, KKT_target_vector)\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Decompose delta to Δx and Δλ\n",
    "    ### TO-DO: Decompose delta to Δx and Δλ\n",
    "    DeltaX = DeltaZ[:2]\n",
    "    DeltaL = DeltaZ[2]\n",
    "    ### END of TO-DO\n",
    "\n",
    "    return x0 + DeltaX, l0 + DeltaL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bbd5-787e-4086-b976-74ab0419acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot things\n",
    "plt.close() # close previous\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 40\n",
    "x1 = np.linspace(-4., 4., N)\n",
    "x2 = np.linspace(-4., 4., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "ax.contour(x1.reshape((N,)), x2.reshape((N,)), val, 50)\n",
    "\n",
    "x = np.linspace(-3.2, 1.2, 1000)\n",
    "y = x**2 + 2. * x\n",
    "ax.plot(x, y, label='c(x)', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a864c5-a1b9-4e84-8b31-fb532fcb52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start optimizing\n",
    "# Initial point!\n",
    "x_init = np.array([[-1., -1.]]).T\n",
    "l_init = np.array([[0.]])\n",
    "ax.plot(x_init[0], x_init[1], 'rx')\n",
    "\n",
    "# Optimization\n",
    "x_new = np.copy(x_init)\n",
    "l_new = np.copy(l_init)\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5a0d1-8452-43c4-bdbe-8220609188f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO: Write the iterate (aka one step) of Newton method for equality constraints\n",
    "x_new, l_new = constrained_newton_step(x_new, l_new)\n",
    "### END of TO-DO\n",
    "\n",
    "ax.plot(x_new[0], x_new[1], 'rx')\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a04518-95c8-4d36-a45a-1ad7b06b6f62",
   "metadata": {},
   "source": [
    "Nice! We can optimize nicely now! With **constraints**! Let's try another initial point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3213c97-4c46-432a-b0f2-030bbe057cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot things\n",
    "plt.close() # close previous\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 40\n",
    "x1 = np.linspace(-4., 4., N)\n",
    "x2 = np.linspace(-4., 4., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "ax.contour(x1.reshape((N,)), x2.reshape((N,)), val)\n",
    "\n",
    "x = np.linspace(-3.2, 1.2, 1000)\n",
    "y = x**2 + 2. * x\n",
    "ax.plot(x, y, label='c(x)', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73272a5e-0dd7-4bc1-a762-b0a29011744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different initial point!\n",
    "x_init = np.array([[-3., 2.]]).T\n",
    "l_init = np.array([[0.]])\n",
    "ax.plot(x_init[0], x_init[1], 'rx')\n",
    "\n",
    "# Optimization\n",
    "x_new = np.copy(x_init)\n",
    "l_new = np.copy(l_init)\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9bf61-4f2d-4773-a58a-ebabcedda689",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO: COPY the iterate (aka one step) of Newton method for equality constraints\n",
    "x_new, l_new = constrained_newton_step(x_new, l_new)\n",
    "### END of TO-DO\n",
    "\n",
    "ax.plot(x_new[0], x_new[1], 'rx')\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fa066-67d0-4e00-92ae-c773ef1fd542",
   "metadata": {},
   "source": [
    "The method is stuck! It cannot converge!! What can we do? Let's add damping/regularization! Remember our talk about *Duality*? We need the **KKT** matrix to have $N$ (dim of $\\boldsymbol{x}$) positive eigenvalues and $M$ (dim of $\\boldsymbol{\\lambda}$) negative ones! This matrix is called **quasi-definite**. So we add regularization accordingly:\n",
    "\n",
    "$\\begin{bmatrix}\\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{x}^2}\\Big|_{\\boldsymbol{x},\\boldsymbol{\\lambda}} + \\beta\\boldsymbol{I} & \\Big(\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}}\\Big)^T\\\\\\frac{\\partial h}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}} & -\\beta\\boldsymbol{I}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\Delta\\boldsymbol{x}\\\\\\Delta\\boldsymbol{\\lambda}\\end{bmatrix} = \\begin{bmatrix}-\\nabla_{\\boldsymbol{x}}\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\lambda})\\\\-h(\\boldsymbol{x})\\end{bmatrix}$\n",
    "\n",
    "Let's write this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed19045-b88a-4107-a0b0-b8d958722634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method with Equality Constraints with Regularization\n",
    "def constrained_newton_step(x0, l0, beta = 1.):\n",
    "    N = 2\n",
    "    M = 1\n",
    "    x0 = np.asarray(x0).reshape((N, -1))\n",
    "    l0 = np.asarray(l0).reshape((M, -1))\n",
    "\n",
    "    # Let's compute the KKT system\n",
    "    ### TO-DO: Copy the KKT matrix implementation from above\n",
    "    ddL_ddx = ddf(x0)\n",
    "    dh_dx = dc(x0)\n",
    "    KKT_system = np.block([[ddL_ddx, dh_dx.T], [dh_dx, 0]])\n",
    "    ### END of TO-DO\n",
    "\n",
    "    ### TO-DO: Implement the above regularization!\n",
    "    KKT_system[:2,:2] += beta * np.eye(2)\n",
    "    KKT_system[2,2] -= beta * 1\n",
    "\n",
    "    eig = np.linalg.eigvals(KKT_system)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    zer_count = 0\n",
    "    for e in eig:\n",
    "        if(e > 0):\n",
    "            pos_count += 1 \n",
    "        if(e < 0):\n",
    "            neg_count += 1\n",
    "            \n",
    "    while(pos_count != 2 and neg_count != 1):\n",
    "        KKT_system[:2,:2] += beta * np.eye(2)\n",
    "        KKT_system[2,2] -= beta * 1\n",
    "        \n",
    "        eig = np.linalg.eigvals(KKT_system)\n",
    "        \n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        zer_count = 0\n",
    "        for e in eig:\n",
    "            if(e > 0):\n",
    "                pos_count += 1\n",
    "            if(e < 0):\n",
    "                neg_count += 1\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Let's compute the target vector\n",
    "    ### TO-DO: Copy the target vector of the KKT system from above\n",
    "    VxL = df(x0).T + dc(x0).T @ l0\n",
    "    h = cons(x0)\n",
    "    KKT_target_vector = np.block([[-VxL], [-h]])\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Let's solve for Δz\n",
    "    ### TO-DO: Solve for Δz\n",
    "    DeltaZ = np.linalg.solve(KKT_system, KKT_target_vector)\n",
    "    ### END of TO-DO\n",
    "\n",
    "    # Decompose delta to Δx and Δλ\n",
    "    ### TO-DO: Decompose delta to Δx and Δλ\n",
    "    DeltaX = DeltaZ[:2]\n",
    "    DeltaL = DeltaZ[2]\n",
    "    ### END of TO-DO\n",
    "\n",
    "    return x0 + DeltaX, l0 + DeltaL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec4ab8-5f06-478a-a090-638a5e571e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try again!\n",
    "plt.close() # close previous\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 40\n",
    "x1 = np.linspace(-4., 4., N)\n",
    "x2 = np.linspace(-4., 4., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "ax.contour(x1.reshape((N,)), x2.reshape((N,)), val, 50)\n",
    "\n",
    "x = np.linspace(-3.2, 1.2, 1000)\n",
    "y = x**2 + 2. * x\n",
    "ax.plot(x, y, label='c(x)', color='orange')\n",
    "\n",
    "# Different initial point!\n",
    "x_init = np.array([[-3., 2.]]).T\n",
    "l_init = np.array([[0.]])\n",
    "ax.plot(x_init[0], x_init[1], 'rx')\n",
    "\n",
    "# Optimization\n",
    "x_new = np.copy(x_init)\n",
    "l_new = np.copy(l_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c46bf-f42f-4b03-a340-3cf440caa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO: COPY the iterate (aka one step) of Newton method for equality constraints\n",
    "x_new, l_new = constrained_newton_step(x_new, l_new)\n",
    "### END of TO-DO\n",
    "\n",
    "ax.plot(x_new[0], x_new[1], 'rx')\n",
    "\n",
    "print(f(x_new))\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce2ac3-5f56-425a-a57e-24d5e564a62c",
   "metadata": {},
   "source": [
    "This is much better!\n",
    "\n",
    "**Re-try the above but remove the term involving the second derivative of the constraints!** Aka, try the **Gauss-Newton** method.\n",
    "\n",
    "You see that it is more robust! This is because even if the Hessian of $f()$ is well-behaved, the term involving the second derivative of the constraints can *spoil the soup*! And it is also usually **expensive to compute**. For these reasons, we usually drop this completely!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b8d67-0793-41d8-a7dd-4214e34b7386",
   "metadata": {},
   "source": [
    "### Constrained Optimization with Inequality Constraints\n",
    "\n",
    "In constrained optimization with inequality constraints we need to solve the following problem:\n",
    "\n",
    "$\\min_{\\boldsymbol{x}} f(\\boldsymbol{x})$\n",
    "\n",
    "$\\quad\\quad\\text{s.t. }g(\\boldsymbol{x})\\leq 0$\n",
    "\n",
    "where $f(\\boldsymbol{x}) : \\mathbb{R}^N\\to\\mathbb{R}, g(\\boldsymbol{x}): \\mathbb{R}^N\\to\\mathbb{R}^M$.\n",
    "\n",
    "The **Lagrangian** of this is:\n",
    "\n",
    "$\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\mu}) = f(\\boldsymbol{x}) + \\boldsymbol{\\mu}^Tg(\\boldsymbol{x})$\n",
    "\n",
    "In this version, we cannot apply the tricks that we did before because we do not have a root finding problem for the constraints!\n",
    "\n",
    "#### Augmented Lagrangian Method\n",
    "\n",
    "We saw in the lectures **Augmented Lagrangian Method**, where we define a new function called **Augmented Lagrangian**:\n",
    "\n",
    "$\\mathcal{L}_{\\rho}(\\boldsymbol{x}, \\boldsymbol{\\mu}) = f(\\boldsymbol{x}) + \\boldsymbol{\\mu}^Tg(\\boldsymbol{x}) + \\frac{\\rho}{2}\\lVert\\text{max}(0, g(\\boldsymbol{x}))\\rVert^2$\n",
    "\n",
    "So we define a procedure as follows:\n",
    "\n",
    "1) $\\boldsymbol{x}_{k+1} = \\min_{\\boldsymbol{x}}\\mathcal{L}_{\\rho}(\\boldsymbol{x}, \\boldsymbol{\\mu}_k)$\n",
    "2) $\\boldsymbol{\\mu}_{k+1} = \\text{max}\\Big(\\boldsymbol{0}, \\boldsymbol{\\mu}_k + \\rho g(\\boldsymbol{x}_{k+1})\\Big)$\n",
    "3) $\\rho = \\alpha\\rho,\\text{ with }\\alpha\\in\\mathbb{R}^{+}$\n",
    "\n",
    "We solve the minimization in step (1) with Newton's method since this is unconstrained minimization in $\\boldsymbol{x}$ (we keep $\\boldsymbol{\\mu}_k$ fixed!). We usually use the **Gauss-Newton** version and:\n",
    "\n",
    "$\\Delta\\boldsymbol{x} = -\\Big(\\nabla^2_{\\boldsymbol{x}}f(\\boldsymbol{x}_k) + \\rho\\Big(\\frac{\\partial g}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Big)^T\\frac{\\partial g}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Big)^{-1}\\nabla_{\\boldsymbol{x}}\\mathcal{L}_{\\rho}(\\boldsymbol{x}_k, \\boldsymbol{\\mu}_k)$\n",
    "\n",
    "Let's implement it? Let's first implement a function that computes the augmented Lagrangian (it will not be actually useful, but it will help your understanding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a2056-8f65-4da3-beab-026c99c84f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def La(x, mu, rho):\n",
    "    ### TO-DO: Compute the Augmented Lagrangian\n",
    "    return f(x) + mu.T @ cons(x) + (rho/2) * np.linalg.norm(np.maximum(0, cons(x)))**2\n",
    "    ### END of TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a1808-94d1-4979-a331-4520200128d6",
   "metadata": {},
   "source": [
    "Now we need to implement step (1) of the Augmented Lagrangian Method, that is the minimization of the augmented Lagrangian over $\\boldsymbol{x}$ alone. Let's do this with Newton's method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9e2ee-f3f2-450b-a2d8-04658e5935f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the optimization routine for La(x,mu,rho) with mu, rho fixed! Running Newton to find the minimum!\n",
    "def newton_solve(x0, mu, rho):\n",
    "    x = np.copy(x0)\n",
    "    x = np.asarray(x).reshape((2,-1))\n",
    "\n",
    "    ### TO-DO: Compute the residual, i.e. d(Lα)/dx\n",
    "    r = (df(x) + mu.T @ dc(x) + rho * np.linalg.norm(np.maximum(0, cons(x))) * dc(x)).T\n",
    "    ### END of TO-DO\n",
    "    while np.linalg.norm(r) >= 1e-8:\n",
    "        ### TO-DO: Compute Hessian of La (Gauss-Newton, we skip the second derivatives of the constraints)\n",
    "        H = ddf(x) + rho * (dc(x).T @ dc(x)) \n",
    "        ### END of TO-DO\n",
    "\n",
    "        # Compute Δx\n",
    "        DeltaX = -np.linalg.solve(H, r)\n",
    "\n",
    "        # Step iterate x_k\n",
    "        x = x + DeltaX\n",
    "\n",
    "        ### TO-DO: Recompute the residual, i.e. d(Lα)/dx (just copy the above)\n",
    "        r = (df(x) + mu.T @ dc(x) + rho * np.linalg.norm(np.maximum(0, cons(x))) * dc(x)).T\n",
    "        ### END of TO-DO\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e6e73-f61b-4a3c-a200-393867e84e5f",
   "metadata": {},
   "source": [
    "Let's see what it does in our test function. Now we interpret the constraints as $g(\\boldsymbol{x})\\leq 0$ instead of equality ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1b5d4-8b08-48f3-a73c-f513b68aca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot things\n",
    "plt.close() # close previous\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 40\n",
    "x1 = np.linspace(-4., 4., N)\n",
    "x2 = np.linspace(-4., 4., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "ax.contour(x1.reshape((N,)), x2.reshape((N,)), val)\n",
    "\n",
    "x = np.linspace(-3.2, 1.2, 1000)\n",
    "y = x**2 + 2. * x\n",
    "ax.plot(x, y, label='c(x)', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cea2ec-f0bb-4031-acf3-35e99c7550dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial point!\n",
    "x_init = np.array([[-3., 2.]]).T\n",
    "mu_init = np.array([[0.]])\n",
    "rho = 1.\n",
    "ax.plot(x_init[0], x_init[1], 'rx')\n",
    "\n",
    "# Optimization\n",
    "x_new = np.copy(x_init)\n",
    "mu_new = np.copy(mu_init)\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30673c-89de-4580-be39-e3ac07bf9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO: Implement step (1), (2), and (3) of ALM\n",
    "alpha = 10.\n",
    "x_new = newton_solve(x_new, mu_new, rho)\n",
    "mu_new = np.maximum(np.array([[0.]]), mu_new + rho * cons(x_new))\n",
    "rho = alpha * rho\n",
    "### END of TO-DO\n",
    "\n",
    "ax.plot(x_new[0], x_new[1], 'rx')\n",
    "\n",
    "print(cons(x_new))\n",
    "print(f(x_new))\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b85491-7c5a-499a-98c6-6d9d4fad962e",
   "metadata": {},
   "source": [
    "### Quadratic Programming\n",
    "\n",
    "A special case of constrained optimization that is very useful in control and robotics is the **Quadratric Programming** (QP) problem:\n",
    "\n",
    "$\\min_{\\boldsymbol{x}}f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q}^T\\boldsymbol{x}$\n",
    "\n",
    "$\\quad\\quad\\text{s.t. }~~\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b} = \\boldsymbol{0}$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\boldsymbol{C}\\boldsymbol{x}-\\boldsymbol{d}\\leq\\boldsymbol{0}$\n",
    "\n",
    "where $\\boldsymbol{x},\\boldsymbol{q}\\in\\mathbb{R}^N$, $\\boldsymbol{Q}\\succ 0\\in\\mathbb{R}^{N\\times N}$.\n",
    "\n",
    "We can solve this special case algorithm with the **Augmented Lagrangian**, but there have been developed many specific methods that optimally exploit the specific structure of the problem. There are many libraries available to use. We will use the [ProxSuite](https://github.com/Simple-Robotics/proxsuite) one that uses a state of the art **proximal method** (based on the Augmented Lagrangian) to solve QP problems.\n",
    "\n",
    "*ProxSuite* solves problems of the form:\n",
    "\n",
    "$\\min_{\\boldsymbol{x}}f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q}^T\\boldsymbol{x}$\n",
    "\n",
    "$\\quad\\quad\\text{s.t. }~~\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b} = \\boldsymbol{0}$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\boldsymbol{l}\\leq\\boldsymbol{C}\\boldsymbol{x}\\leq\\boldsymbol{u}$\n",
    "\n",
    "So let's define a QP problem and try to solve it with the *ProxSuite* library. First, let's import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb0910-25b3-4006-a6b4-285451b7555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import proxsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03f064-ed37-4f3e-9a7f-a3032b07b0ac",
   "metadata": {},
   "source": [
    "Now let's generate a random QP problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790e50b-e0d3-42aa-983a-02b6b86002e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random non-trivial quadratic program.\n",
    "n = 5 # number of dimensions of the optimization variables (x)\n",
    "m = 2 # number of inequality constraints\n",
    "p = 1 # number of equality constraints\n",
    "\n",
    "# Random Cost\n",
    "Q = np.random.randn(n, n)\n",
    "Q = Q.T @ Q # We need to make sure that Q is positive definite\n",
    "q = np.random.randn(n)\n",
    "\n",
    "# Random inequality constraints\n",
    "C = np.random.randn(m, n)\n",
    "u = C @ np.random.randn(n)\n",
    "\n",
    "# Random equality constraints\n",
    "A = np.random.randn(p, n)\n",
    "b = np.random.randn(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e1e1e-ac04-4585-83f6-8ccd6658ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the solver\n",
    "qp_dim = Q.shape[0]\n",
    "qp_dim_eq = A.shape[0]\n",
    "qp_dim_in = C.shape[0]\n",
    "qp = proxsuite.proxqp.dense.QP(qp_dim, qp_dim_eq, qp_dim_in)\n",
    "\n",
    "# initialize the model of the problem to solve\n",
    "qp.init(Q, q, A, b, C, u, None)\n",
    "qp.solve()\n",
    "\n",
    "# Get the result\n",
    "print(\"optimal x: {}\".format(qp.results.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afe918-43f1-40c2-8da2-921d489b0838",
   "metadata": {},
   "source": [
    "So now we can easily define QP problems, give them to *ProxSuite* and get back the results! Easy!\n",
    "\n",
    "### General Non-Linear Programming (NLP)\n",
    "\n",
    "What if we have a general non linear problem with constraints that we want to minimize/optimize? We can implement the **Augmented Lagrangian Method** but in practice many small tricks are needed to have a robust solver. So what should we do? Luckily, there are quite a few good libraries out there for solving generic NLP problems. In this course, we will use [Ipopt](https://github.com/coin-or/Ipopt).\n",
    "\n",
    "Ipopt solves generic NLP problems with any type of objective functions and constraints. We need to provide to Ipopt:\n",
    "\n",
    "1) The objective function and its gradient\n",
    "2) The constraints and their Jacobians\n",
    "3) Bounds to the variables\n",
    "\n",
    "Then Ipopt does all the job and we get back a nice result (most of the times!). In the backed, Ipopt uses a **Primal-Dual Interior Point Method**.\n",
    "\n",
    "So let's see it in action! First we import the Python bindings of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be76198-4383-4a7b-aae4-27fc20344012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyipopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbfc9c-9e47-4c93-b054-8e71b30e236f",
   "metadata": {},
   "source": [
    "Let's optimize the following function:\n",
    "\n",
    "$f(\\boldsymbol{x}) = (x_1-2)^2 + (x_2-1)^2$\n",
    "\n",
    "with the following constraints:\n",
    "\n",
    "$h(\\boldsymbol{x}) = x_1-2\\,x_2+1=0$\n",
    "\n",
    "$g(\\boldsymbol{x}) = 0.25\\,{x_1^2}+x_2^2-1\\leqslant 0$\n",
    "\n",
    "The best known optimal feasible solution is $f(\\boldsymbol{x^*}) = 1.3934651$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914efcd5-bf3c-4a86-9674-d724543e2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here's how to write this for cyipopt\n",
    "class IpoptOptimization:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def objective(self, x):\n",
    "        return (x[0] - 2.)**2 + (x[1]-1.)**2\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.asarray([2. * (x[0] - 2.), 2. * (x[1] - 1.)])\n",
    "\n",
    "    def constraints(self, x):\n",
    "        c = np.zeros((2,))\n",
    "\n",
    "        # Equality\n",
    "        c[0] = x[0] - 2. * x[1] + 1.\n",
    "        # Inequality\n",
    "        c[1] = 0.25 * x[0] * x[0] + x[1] * x[1] - 1.\n",
    "\n",
    "        return c\n",
    "\n",
    "    def jacobian(self, x):\n",
    "        J = np.zeros((2, 2))\n",
    "        # Gradient of 1st constraint wrt to x1\n",
    "        J[0, 0] = 1.\n",
    "        # Gradient of 1st constraint wrt to x2\n",
    "        J[0, 1] = -2.\n",
    "\n",
    "        # Gradient of 2nd constraint wrt to x1\n",
    "        J[1, 0] = 0.5 * x[0]\n",
    "        # Gradient of 2nd constraint wrt to x2\n",
    "        J[1, 1] = 2. * x[1]\n",
    "        \n",
    "        return J\n",
    "\n",
    "# Let's give a zero initial guess\n",
    "x0 = np.zeros((2,))\n",
    "\n",
    "# Bounds for constraints\n",
    "cl = np.zeros((2,))\n",
    "cl[1] = -np.inf # Inequality has no lower bound\n",
    "\n",
    "cu = np.zeros((2,))\n",
    "\n",
    "# Let's solve the problem\n",
    "nlp = cyipopt.Problem(n=2, m=2, problem_obj=IpoptOptimization(), lb=[None]*2, ub=[None]*2, cl=cl, cu=cu)\n",
    "\n",
    "nlp.add_option(\"jacobian_approximation\", \"exact\") # \"finite-difference-values\")\n",
    "nlp.add_option(\"print_level\", 3)\n",
    "nlp.add_option(\"nlp_scaling_method\", \"none\")\n",
    "\n",
    "\n",
    "# Solve the problem\n",
    "x, info = nlp.solve(x0)\n",
    "\n",
    "# Optimal\n",
    "print(x)\n",
    "print(info['obj_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0902ccd-670f-4f84-a0da-51a45019dc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
